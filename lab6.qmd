---
title: "Lab 6: Machine Learning"
format:
  html:
    self-contained: true
---

Lab Set Up
```{r}
library(tidyverse)
library(tidymodels)
library(powerjoin)
library(glue)
library(vip)
library(baguette)

root  <- 'https://gdex.ucar.edu/dataset/camels/file'

download.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', 
              'data/camels_attributes_v2.0.pdf')

types <- c("clim", "geol", "soil", "topo", "vege", "hydro")

```
Getting Basin Characteristics
```{r}
remote_files  <- glue('{root}/camels_{types}.txt')
local_files   <- glue('data/camels_{types}.txt')

walk2(remote_files, local_files, download.file, quiet = TRUE)

camels <- map(local_files, read_delim, show_col_types = FALSE)

camels <- power_full_join(camels ,by = 'gauge_id')
```

Question 1
```{r}
ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = q_mean)) +
  scale_color_gradient(low = "pink", high = "dodgerblue") +
  ggthemes::theme_map()
```
The documentation PDF tells us that zero_q_freq is the frequency of days with Q=0 mm/day.


Question 2
```{r}
#EDA
camels |> 
  select(aridity, p_mean, q_mean) |> 
  drop_na() |> 
  cor()

# Create a scatter plot of aridity vs rainfall
ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm", color = "red", linetype = 2) +
  scale_color_viridis_c() +
  theme_linedraw() + 
  theme(legend.position = "bottom") + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow")
```

```{r}
#Log Transformation
ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm") +
  scale_color_viridis_c() +
  # Apply log transformations to the x and y axes
  scale_x_log10() + 
  scale_y_log10() +
  theme_linedraw() +
  theme(legend.position = "bottom") + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow")

#Log Transforming the color scale
ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm") +
  # Apply a log transformation to the color scale
  scale_color_viridis_c(trans = "log") +
  scale_x_log10() + 
  scale_y_log10() +
  theme_linedraw() +
  theme(legend.position = "bottom",
        # Expand the legend width ...
        legend.key.width = unit(2.5, "cm"),
        legend.key.height = unit(.5, "cm")) + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow") 
```

```{r}
set.seed(123)

camels <- camels |> 
  mutate(logQmean = log(q_mean))

# Generate the split
camels_split <- initial_split(camels, prop = 0.8)
camels_train <- training(camels_split)
camels_test  <- testing(camels_split)

camels_cv <- vfold_cv(camels_train, v = 10)

# Create a recipe to preprocess the data
rec <-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %>%
  # Log transform the predictor variables (aridity and p_mean)
  step_log(all_predictors()) %>%
  # Add an interaction term between aridity and p_mean
  step_interact(terms = ~ aridity:p_mean) |> 
  # Drop any rows with missing values in the pred
  step_naomit(all_predictors(), all_outcomes())


baked_data <- prep(rec, camels_train) |> 
  bake(new_data = NULL)

#  Base lm sets interaction terms with the * symbol
lm_base <- lm(logQmean ~ aridity * p_mean, data = baked_data)
summary(lm_base)

summary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))
```

```{r}
#Predicting
test_data <-  bake(prep(rec), new_data = camels_test)
test_data$lm_pred <- predict(lm_base, newdata = test_data)

metrics(test_data, truth = logQmean, estimate = lm_pred)
ggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +
  scale_color_gradient2(low = "brown", mid = "orange", high = "darkgreen") +
  geom_point() +
  geom_abline(linetype = 2) +
  theme_linedraw() + 
  labs(title = "Linear Model: Observed vs Predicted",
       x = "Observed Log Mean Flow",
       y = "Predicted Log Mean Flow",
       color = "Aridity")
```

```{r}
#Using a workflow
# Define model
lm_model <- linear_reg() %>%
  # define the engine
  set_engine("lm") %>%
  # define the mode
  set_mode("regression")

# Instantiate a workflow ...
lm_wf <- workflow() %>%
  # Add the recipe
  add_recipe(rec) %>%
  # Add the model
  add_model(lm_model) %>%
  # Fit the model to the training data
  fit(data = camels_train) 

# Extract the model coefficients from the workflow
summary(extract_fit_engine(lm_wf))$coefficients

summary(lm_base)$coefficients
```

```{r}
#Making Predictions
lm_data <- augment(lm_wf, new_data = camels_test)
dim(lm_data)

metrics(lm_data, truth = logQmean, estimate = .pred)

ggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

```{r}
#Using a random forest model
rf_model <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

rf_wf <- workflow() %>%
  # Add the recipe
  add_recipe(rec) %>%
  # Add the model
  add_model(rf_model) %>%
  # Fit the model
  fit(data = camels_train) 

rf_data <- augment(rf_wf, new_data = camels_test)
dim(rf_data)

metrics(rf_data, truth = logQmean, estimate = .pred)

ggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()
```

```{r}
#Workflowset
wf <- workflow_set(list(rec), list(lm_model, rf_model)) %>%
  workflow_map('fit_resamples', resamples = camels_cv) 

autoplot(wf)

rank_results(wf, rank_metric = "rsq", select_best = TRUE)
```

Question 3
```{r}
#Creating an xgboost regression model
set.seed(123)
camel_split <- initial_split(camels, prop = 0.8)
camel_train <- training(camel_split)
camel_test  <- testing(camel_split)
camel_folds <- vfold_cv(camel_train, v = 10)

camel_recipe <- recipe(aridity ~ ., data = camel_train)  |> 
  step_dummy(all_nominal_predictors())  |> 
  step_normalize(all_numeric_predictors()) |> 
  step_impute_mean(all_numeric_predictors())

b_model <- boost_tree() |>
  set_engine("xgboost") |>
  set_mode("classification")

b_model <- boost_tree(mode = "regression") %>%
  set_engine("xgboost")
```

```{r}
#Building a neural network model
nn_model <- bag_mlp(hidden_units = 5, penalty = 0.01) |> 
  set_engine("nnet") |> 
  set_mode("classification")

nn_model <- bag_mlp(mode = "regression") %>%
  set_engine("nnet", times = 25)
```

```{r}
#Workflow
wf <- workflow_set(list(rec), list(lm_model, rf_model, nn_model, b_model)) %>%
  workflow_map('fit_resamples', resamples = camels_cv) 

autoplot(wf)

rank_results(wf, rank_metric = "rsq", select_best = TRUE)
```
Looking at the r squared value for all four models, the neural network model using the bag_mlp function has the highest value. Therefore it is the best model and the one I would choose to move forward with. 


Build your own!
```{r}
#Data Splitting
camels2 <- camels |>
  mutate(logQmean = log(q_mean)) |>
  select(logQmean, p_mean, aridity, soil_depth_pelletier, max_water_content, organic_frac, frac_snow, pet_mean, soil_depth_statsgo, elev_mean, slope_mean, area_gages2) |>
  na.omit()

set.seed(123)
camel_split2 <- initial_split(camels2, prop = 0.75)
camel_train2 <- training(camel_split2)
camel_test2  <- testing(camel_split2)
camel_folds2 <- vfold_cv(camel_train2, v = 10)
```

```{r}
#Recipe
rec_camel <-  recipe(logQmean ~ p_mean + pet_mean + elev_mean + area_gages2 + max_water_content + slope_mean, data = camel_train2) %>%
  step_scale(all_predictors()) |>
  step_center(all_predictors())
```
I chose this formula because the chosen variables are the ones that influence stream flow from the downloaded PDF. I feel that the chosen variables will have a statistically significant correlation to stream flow. 

```{r}
#Defining the models
rf_model_camel <- rand_forest() %>%
  set_engine("ranger") %>%
  set_mode("regression")

xgb_camel <- boost_tree(mode = "regression") %>%
  set_engine("xgboost")

dt_camel <- decision_tree(mode = "regression") |>
  set_engine("rpart")
```

```{r}
#Workflow Set
wf_2 <- workflow_set(list(rec_camel), list(xgb_camel, dt_camel, rf_model_camel)) %>%
  workflow_map('fit_resamples', resamples = camels_cv) 

autoplot(wf_2)
```
The random forest model is the best fit for the CAMEL data set. We can come to this conclusion by looking at the r squared values on the autoplot and can see that the random forest model has the highest one, meaning it will do the best job for this data set. 

```{r}
#Extract and Evaluate
rf_wf_camel <- workflow() %>%
  add_recipe(rec_camel) %>%
  add_model(rf_model_camel) %>%
  fit(data = camel_train2) 

rf_data2 <- augment(rf_wf_camel, new_data = camel_test2)
dim(rf_data2)

ggplot(rf_data2, aes(x = .pred, y = logQmean)) +
  geom_point(color = "skyblue") +
  geom_abline() +
  theme_linedraw() +
  labs(title = "Observed vs. Predicted LogQmean Values",
       x = "Predicted Values",
       y = "Observed Values")
```

The observed and predicted values follow a linear relationship, leading me to determine that there is a correlation between my chosen variables and the log Q mean of the stream flow. 
